{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m476.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=d85d1b429214b98cf63ed2993fb4d107a693e37eb2cb8f60bad810f03834ee8a\n",
      "  Stored in directory: /home/carlos/.cache/pip/wheels/b1/91/5f/283b53010a8016a4ff1c4a1edd99bbe73afacb099645b5471b\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/23 12:08:29 WARN Utils: Your hostname, carlos-HP-EliteBook-850-G3 resolves to a loopback address: 127.0.1.1; using 192.168.1.102 instead (on interface wlp2s0)\n",
      "24/06/23 12:08:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/23 12:08:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/23 12:08:45 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# SparkSession es el punto de entrada para interactuar\n",
    "# con Spark y realizar operaciones de procesamiento\n",
    "# de datos. Reemplaza a SparkContext y SQLContext en\n",
    "# versiones anteriores de Spark.\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EjemploPySpark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|Nombre|Edad|\n",
      "+------+----+\n",
      "| Alice|  34|\n",
      "|   Bob|  45|\n",
      "| Cathy|  29|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame es una colección distribuida de datos\n",
    "# organizados en columnas, similar a una\n",
    "# tabla en una base de datos relacional o\n",
    "# a un dataframe en pandas. Los DataFrames son inmutables.\n",
    "\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)]\n",
    "columns = [\"Nombre\", \"Edad\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "# RDD es la estructura de datos fundamental de Spark que\n",
    "# representa una colección distribuida de elementos.\n",
    "# Es inmutable y puede ser creada a partir de archivos \n",
    "# en el sistema de archivos Hadoop (HDFS) o desde colecciones en el programa principal de Python.\n",
    "\n",
    "\n",
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "# Transformaciones: Operaciones que crean un nuevo RDD\n",
    "# a partir de uno existente, son perezosas \n",
    "# (lazy) y no ejecutan el código inmediatamente.\n",
    "\n",
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "rdd2 = rdd.map(lambda x: x * 2)\n",
    "\n",
    "# Acciones: Operaciones que devuelven un valor al programa principal o exportan datos a\n",
    "# un sistema de almacenamiento. Ejecutan las transformaciones pendientes en el RDD.\n",
    "\n",
    "result = rdd2.collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Nombre|\n",
      "+------+\n",
      "| Alice|\n",
      "|   Bob|\n",
      "| Cathy|\n",
      "+------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Select: Seleccionar columnas específicas.\n",
    "print(df.select(\"Nombre\").show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|Nombre|Edad|\n",
      "+------+----+\n",
      "| Alice|  34|\n",
      "|   Bob|  45|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter: Filtrar filas basadas en una condición.\n",
    "df.filter(df[\"Edad\"] > 30).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|Nombre|max(Edad)|\n",
      "+------+---------+\n",
      "| Alice|       34|\n",
      "|   Bob|       45|\n",
      "| Cathy|       29|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GroupBy: Agrupar filas por una columna y realizar agregaciones.\n",
    "df.groupBy(\"Nombre\").max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+\n",
      "|Nombre|Edad|Género|\n",
      "+------+----+------+\n",
      "| Alice|  34|     F|\n",
      "|   Bob|  45|     M|\n",
      "| Cathy|  29|     F|\n",
      "+------+----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Join: Combinar DataFrames basados en una clave común.\n",
    "data2 = [(\"Alice\", \"F\"), (\"Bob\", \"M\"), (\"Cathy\", \"F\")]\n",
    "columns2 = [\"Nombre\", \"Género\"]\n",
    "df2 = spark.createDataFrame(data2, columns2)\n",
    "df.join(df2, \"Nombre\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+--------+\n",
      "|Nombre|Edad|Edad + 3|\n",
      "+------+----+--------+\n",
      "| Alice|  34|      37|\n",
      "|   Bob|  45|      48|\n",
      "| Cathy|  29|      32|\n",
      "+------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Funciones UDF (User Defined Functions)\n",
    "from pyspark.sql.functions import udf \n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def suma(x):\n",
    "    return x + 3\n",
    "\n",
    "suma_udf = udf(suma, IntegerType())\n",
    "\n",
    "df.withColumn(\"Edad + 3\", suma(df[\"Edad\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controlar el número de particiones para mejorar el rendimiento.\n",
    "rdd = rdd.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Nombre: string, Edad: bigint]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cache/Persist: Almacenar datos en memoria para mejorar el\n",
    "# rendimiento en operaciones repetidas.\n",
    "print(f\"Cache \\n{df.cache()}\")\n",
    "\n",
    "# EXPLAIN\n",
    "# Muestra el plan de ejecución\n",
    "print(f\"Explain \\n{df.explain()}\")\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
